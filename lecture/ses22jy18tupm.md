---
layout: page
title: Session.22-July 18-Tuesday aft
nav_exclude: true
---
# Session 22- Tuesday, July 18 - Afternoon
- 305 Davis Hall
- 1pm; Arrival of new video equipment for interviews at CSI: Rode microphone for interviews

- Discussion of costs of forming queries; Costs of embedding texts and PDF's
- PipeChat surpassed $100 a day for past several days, hit ceiling;


- DR will make PipeChat to work again, using GPT-4 queries. We should use a different channel for experiments, so not to overwhelm this channel. 

DR  briefly outlined  progress in building a new structure for CSIChat.  It's a *hybrid* model, using vector embedding for queries and context, and LLM for long query response. Iterate, using LLM model response to tune the query, search more relevant document embeddings.

A question generates a search locally, tuned to the text in the local database, to find the most relevant text from CSI or Sustainability that is in the local database.  Relevance is computed by doing one pass through GPT-4, or through another LLM running locally, that uses the local vector database that was built by ingesting and  classifying the local text. 

Then, this *relevant* text goes into the GPT-4 playground context space, and the  GPT-4 query  runs,  this time using this most relevant text as context.  The entire query  and context goes to GPT-4 for a much richer response than what  could be built from just the local CSI or Sustainability text.  And a great side effect is that the query answer now can be directly linked back to the original CSI or Sustainability text that formed context for the query. As we explore the validity of the responses, this helps a lot.

DR purchased a MacBook Pro with huge memory and GPU to use as the class computing node. He installed PostGres  on it as the database to hold all the documents from CSI, Soga Research Group,  and the Sustainability materials. He installed a vector database on it to hold the embeddings. The embeddings are  generated by a new piece of open-source software, which  uses a state of the art open source LLM, and also uses links to GPT-4, to embed the text that will be the basis for  the final question and answer materials.  

So the original materials, text-only for the moment, go into PostGres on the MAC. They  are ranked for relevance by the new tokenizing and vectorizing or embedding software running on the Mac. 

When a query is made,  text that is relevant to that query from CSI or Sustainability will be pulled from the PostGres database, and that smaller amount of text will then go into the GPT-4 Playground as context.  It can be controlled to be less than the 8,000 token limit. 

Then GPT-4 generates the response, constrained to the context text but with relevant answers from  the rest of GPT-4. That machinery generates the response that we  then can call **CSIChat**, or **SRGChat**, or  SogaChat, PIpeChat, or **SustainabilityChat**.

The past implementation using llama-index in the Jupyter notebook in Colab has been yielding incorrect results afer indexing text from CSI researchers, because it only indexes one sentence at at time, so there's too little to use by GPT-4 to avoid hallucinations.

DR explained that llama-index only used GPT-3.5, with a small search window. He suggested using the GPT-4 Playground, but noted that large context windows pushed it beyond its capacity for tokens, so it could not respond to a query. 

This new architecture is designed to get around this, for the moment.  As we get more computer power, we can do better initial embeddings, with longer and more detailed vectors capturing the meaning, or semantics, of our source material. 

Major new development: using new tools to shape the "contexts" in queries are generating much more accurate responses from the LLM's.  Improvements of ten or twelve percent on Hugging-faces tests. 


